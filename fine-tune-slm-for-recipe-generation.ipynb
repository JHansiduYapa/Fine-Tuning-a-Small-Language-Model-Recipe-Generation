{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":48945683,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U accelerate\n%pip install -U peft\n%pip install -U trl\n%pip install kagglehub[hf-datasets]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:05:20.475571Z","iopub.execute_input":"2025-07-04T17:05:20.475882Z","iopub.status.idle":"2025-07-04T17:06:06.934798Z","shell.execute_reply.started":"2025-07-04T17:05:20.475834Z","shell.execute_reply":"2025-07-04T17:06:06.929670Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install ipywidgets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:06:48.827079Z","iopub.execute_input":"2025-07-04T17:06:48.827419Z","iopub.status.idle":"2025-07-04T17:06:53.424176Z","shell.execute_reply.started":"2025-07-04T17:06:48.827391Z","shell.execute_reply":"2025-07-04T17:06:53.419945Z"}},"outputs":[{"name":"stdout","text":"Collecting ipywidgets\n  Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\nRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (8.36.0)\nCollecting jupyterlab_widgets~=3.0.15\n  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\nCollecting widgetsnbextension~=4.0.14\n  Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\nRequirement already satisfied: stack_data in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\nRequirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\nRequirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\nRequirement already satisfied: typing_extensions>=4.6 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.13.2)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\nRequirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\nRequirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\nRequirement already satisfied: pure-eval in /usr/local/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\nInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\nSuccessfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(new_session=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:06:53.426769Z","iopub.execute_input":"2025-07-04T17:06:53.427084Z","iopub.status.idle":"2025-07-04T17:06:53.525728Z","shell.execute_reply.started":"2025-07-04T17:06:53.427052Z","shell.execute_reply":"2025-07-04T17:06:53.521006Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"474ee9739e9b470fa75d7457db492a25"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"### 1. Install relevant libraries\n\nInstall the libraries to fine-tune the task.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer\nfrom trl import setup_chat_format\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\nfrom sklearn.metrics import (accuracy_score, \n                             classification_report, \n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:07:15.457620Z","iopub.execute_input":"2025-07-04T17:07:15.457966Z","iopub.status.idle":"2025-07-04T17:08:18.189725Z","shell.execute_reply.started":"2025-07-04T17:07:15.457937Z","shell.execute_reply":"2025-07-04T17:08:18.183572Z"}},"outputs":[{"name":"stderr","text":"The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:251: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1751648882.490982      10 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:230\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model_id = \"google/gemma-2-2b\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:08:22.867935Z","iopub.execute_input":"2025-07-04T17:08:22.868578Z","iopub.status.idle":"2025-07-04T17:08:22.881115Z","shell.execute_reply.started":"2025-07-04T17:08:22.868546Z","shell.execute_reply":"2025-07-04T17:08:22.874274Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:08:23.945676Z","iopub.execute_input":"2025-07-04T17:08:23.946035Z","iopub.status.idle":"2025-07-04T17:08:29.702105Z","shell.execute_reply.started":"2025-07-04T17:08:23.946005Z","shell.execute_reply":"2025-07-04T17:08:29.697999Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# This tokenizer is aware of the start of the turn and the end of the turn\ntext = \"<start_of_turn><end_of_turn>\"\ntokens = tokenizer.tokenize(text)\nprint(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:08:34.023948Z","iopub.execute_input":"2025-07-04T17:08:34.024290Z","iopub.status.idle":"2025-07-04T17:08:34.044530Z","shell.execute_reply.started":"2025-07-04T17:08:34.024260Z","shell.execute_reply":"2025-07-04T17:08:34.037896Z"}},"outputs":[{"name":"stdout","text":"['<start_of_turn>', '<end_of_turn>']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\n\n# load csv file as a pandas dataframe \n# then split the dataset \n# Load CSV file into a Pandas DataFrame\ndf = pd.read_csv(\"/kaggle/input/explore-recipe-nlg-dataset/recipe_df.csv\")\n\n# Select the first 10,000 rows\ndf_subset = df.iloc[:10000].copy()\n\n# shuffle and split: 2000 samples for test, rest for train\ntrain_df, test_df = train_test_split(df_subset, test_size=2000, random_state=42, shuffle=True)\n\n# reset index\ntrain_df = train_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\n\n# convert to the huggingface dataset\ntrain_ds = Dataset.from_pandas(train_df, split=\"train\")\ntest_ds = Dataset.from_pandas(test_df, split=\"test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:08:35.413154Z","iopub.execute_input":"2025-07-04T17:08:35.413493Z","iopub.status.idle":"2025-07-04T17:09:23.177347Z","shell.execute_reply.started":"2025-07-04T17:08:35.413465Z","shell.execute_reply":"2025-07-04T17:09:23.171394Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### 2. Make a prompt template \n\nApply the prompt template to the dataset\n\nThen remove the unnecessary columns from the dataset","metadata":{}},{"cell_type":"code","source":"import ast\n\ndef generate_prompt(data_point):\n    try:\n        ner = ast.literal_eval(data_point[\"NER\"]) if isinstance(data_point[\"NER\"], str) else data_point[\"NER\"]\n        ingredients = ast.literal_eval(data_point[\"ingredients\"]) if isinstance(data_point[\"ingredients\"], str) else data_point[\"ingredients\"]\n        directions = ast.literal_eval(data_point[\"directions\"]) if isinstance(data_point[\"directions\"], str) else data_point[\"directions\"]\n    except Exception as e:\n        print(\"Parsing error:\", e)\n        ner, ingredients, directions = [], [], []\n\n    input_ingredients = \"\\n\".join(ner)\n    ingredients_text = \"\\n\".join(ingredients)\n    directions_text = \"\\n\".join(directions)\n\n    prompt = (\n        \"<start_of_turn>user\\n\"\n        \"Below is a list of ingredients. Write a complete recipe using them.\\n\\n\"\n        f\"{input_ingredients}\\n\"\n        \"<end_of_turn>\\n\"\n    )\n\n    completion = (\n        \"<start_of_turn>model\\n\"\n        f\"Title: {data_point['title']}\\n\"\n        f\"Ingredients:\\n{ingredients_text}\\n\"\n        f\"Directions:\\n{directions_text}\\n\"\n        \"<end_of_turn>\"\n    )\n\n    return {\"prompt\": prompt, \"completion\": completion}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:09:42.801142Z","iopub.execute_input":"2025-07-04T17:09:42.801422Z","iopub.status.idle":"2025-07-04T17:09:42.817722Z","shell.execute_reply.started":"2025-07-04T17:09:42.801398Z","shell.execute_reply":"2025-07-04T17:09:42.812209Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# convert to the correct prompt\n# Apply to both datasets\ntrain_dataset = train_ds.map(generate_prompt)\ntest_dataset = test_ds.map(generate_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:09:46.174618Z","iopub.execute_input":"2025-07-04T17:09:46.174950Z","iopub.status.idle":"2025-07-04T17:09:48.165820Z","shell.execute_reply.started":"2025-07-04T17:09:46.174913Z","shell.execute_reply":"2025-07-04T17:09:48.162395Z"}},"outputs":[{"name":"stderr","text":"Map: 100%|██████████| 8000/8000 [00:01<00:00, 5047.91 examples/s]\nMap: 100%|██████████| 2000/2000 [00:00<00:00, 5215.97 examples/s]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(test_dataset)\nprint(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:09:49.671464Z","iopub.execute_input":"2025-07-04T17:09:49.671734Z","iopub.status.idle":"2025-07-04T17:09:49.680781Z","shell.execute_reply.started":"2025-07-04T17:09:49.671709Z","shell.execute_reply":"2025-07-04T17:09:49.676209Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['Unnamed: 0', 'title', 'ingredients', 'directions', 'link', 'source', 'NER', 'prompt', 'completion'],\n    num_rows: 2000\n})\nDataset({\n    features: ['Unnamed: 0', 'title', 'ingredients', 'directions', 'link', 'source', 'NER', 'prompt', 'completion'],\n    num_rows: 8000\n})\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"train_dataset[\"prompt\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:09:51.390731Z","iopub.execute_input":"2025-07-04T17:09:51.391053Z","iopub.status.idle":"2025-07-04T17:09:51.415631Z","shell.execute_reply.started":"2025-07-04T17:09:51.391029Z","shell.execute_reply":"2025-07-04T17:09:51.410151Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'<start_of_turn>user\\nBelow is a list of ingredients. Write a complete recipe using them.\\n\\nwhite bread\\ncream cheese\\nvanilla\\nmilk\\nbutter\\nsugar\\ncinnamon\\n<end_of_turn>\\n'"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"### 3. Load the model for training \n\nLoad the model for finetuning, and get the model quantized.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:09:56.626187Z","iopub.execute_input":"2025-07-04T17:09:56.626467Z","iopub.status.idle":"2025-07-04T17:11:36.345008Z","shell.execute_reply.started":"2025-07-04T17:09:56.626444Z","shell.execute_reply":"2025-07-04T17:11:36.339464Z"}},"outputs":[{"name":"stderr","text":"Fetching 3 files: 100%|██████████| 3/3 [00:09<00:00,  3.30s/it]\nLoading checkpoint shards: 100%|██████████| 3/3 [01:26<00:00, 28.98s/it]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### 4.Step 4 - Apply Lora(Low-Rank Adaptation)\n\nIt's a parameter-efficient fine-tuning technique that reduces the number of trainable parameters during the fine-tuning process. \n\nInstead of updating all the weights of the original model, LoRA adds small, trainable matrices (low-rank matrices) to the existing model layers.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:11:58.063153Z","iopub.execute_input":"2025-07-04T17:11:58.063448Z","iopub.status.idle":"2025-07-04T17:11:58.372859Z","shell.execute_reply.started":"2025-07-04T17:11:58.063423Z","shell.execute_reply":"2025-07-04T17:11:58.367254Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import bitsandbytes as bnb\n\ndef find_all_linear_names(model):\n    # find the modules that can be applied low-rank matrices,\n    # linear layers weight matrix can be divided into row and column matrix low-rank matices for adaptation.\n    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n    \n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n          names = name.split('.')\n          lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n        if 'lm_head' in lora_module_names: # needed for 16-bit\n          lora_module_names.remove('lm_head')\n    \n    return list(lora_module_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:12:00.471797Z","iopub.execute_input":"2025-07-04T17:12:00.472075Z","iopub.status.idle":"2025-07-04T17:12:07.788869Z","shell.execute_reply.started":"2025-07-04T17:12:00.472051Z","shell.execute_reply":"2025-07-04T17:12:07.783627Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"modules = find_all_linear_names(model)\nprint(modules)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:12:07.791402Z","iopub.execute_input":"2025-07-04T17:12:07.791653Z","iopub.status.idle":"2025-07-04T17:12:08.035145Z","shell.execute_reply.started":"2025-07-04T17:12:07.791631Z","shell.execute_reply":"2025-07-04T17:12:08.030612Z"}},"outputs":[{"name":"stdout","text":"['o_proj', 'gate_proj', 'down_proj', 'k_proj', 'v_proj', 'q_proj', 'up_proj']\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# add the peft to the found layers of transformer model\nfrom peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=32,\n    target_modules=modules,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:12:15.576962Z","iopub.execute_input":"2025-07-04T17:12:15.577252Z","iopub.status.idle":"2025-07-04T17:12:16.687491Z","shell.execute_reply.started":"2025-07-04T17:12:15.577229Z","shell.execute_reply":"2025-07-04T17:12:16.682363Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"print(next(model.parameters()).device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:12:16.689645Z","iopub.execute_input":"2025-07-04T17:12:16.689945Z","iopub.status.idle":"2025-07-04T17:12:35.571060Z","shell.execute_reply.started":"2025-07-04T17:12:16.689919Z","shell.execute_reply":"2025-07-04T17:12:35.566630Z"}},"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# find the trainable parameters \ntrainable, total = model.get_nb_trainable_parameters()\nprint(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:12:39.611851Z","iopub.execute_input":"2025-07-04T17:12:39.612184Z","iopub.status.idle":"2025-07-04T17:12:39.629981Z","shell.execute_reply.started":"2025-07-04T17:12:39.612157Z","shell.execute_reply":"2025-07-04T17:12:39.624571Z"}},"outputs":[{"name":"stdout","text":"Trainable: 83066880 | total: 2697408768 | Percentage: 3.0795%\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### Train the model using the SFT trainer\n\nFrom the dataset for the training purpose it will consider only the \"prompt\"\n\ndataset_text_field=\"prompt\",","metadata":{}},{"cell_type":"code","source":"import transformers\nfrom trl import SFTConfig\nfrom trl import SFTTrainer\nfrom transformers import DataCollatorForLanguageModeling\n\n\ntokenizer.pad_token = tokenizer.eos_token\ntorch.cuda.empty_cache()\n\n# add sft config instead of training arguments\nsft_config = SFTConfig(\n    output_dir=\"outputs\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=10,  # set steps num of epochs\n    learning_rate=2e-4,\n    logging_steps=1,\n    save_strategy=\"epoch\",\n    optim=\"paged_adamw_8bit\",  # If using bitsandbytes or quantization\n    dataset_text_field=\"prompt\",\n    no_cuda=False,\n    label_names=[\"labels\"] \n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    peft_config=lora_config, \n    args=sft_config,          \n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T17:12:41.651805Z","iopub.execute_input":"2025-07-04T17:12:41.652189Z","execution_failed":"2025-07-04T17:21:30.075Z"}},"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1751649178.125011      10 common_lib.cc:621] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:232\nAdding EOS to train dataset: 100%|██████████| 8000/8000 [00:00<00:00, 8180.65 examples/s]\nTokenizing train dataset: 100%|██████████| 8000/8000 [00:08<00:00, 984.76 examples/s] \nTruncating train dataset: 100%|██████████| 8000/8000 [00:00<00:00, 164220.08 examples/s]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-04T17:21:30.075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_model = \"gemma2-Recipe-Instruct-Finetune\"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}